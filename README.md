# MIDI Music Generation using Transformers
This project is a MIDI music generation system based on two seperate transformer models. It uses the Maestro dataset for training and generates MIDI files as output.

The key difference between the two transformer models is the embedding representation of the data, which fundamentally changes the important placed on notes and rythm. The first model is a continuous embedding of pitch, time, velocity, and is great at creating good tempo and rythm, whereas the second model tokenizes the input into discrete time slices with discrete notes, whose strength lies in identifying natural choord progression, but is not as good at identifying rythm and tempo.

## Key Files:
**data_loader.py**: This file is responsible for loading the Maestro dataset from the data/maestro-v3.0.0/ directory.

**embedding.py**: This file contains functions for converting MIDI files to and from a format suitable for the transformer model. Key functions include note_seqs for creating sequences of notes from MIDI files, and tensor_features for converting these sequences into tensor format.

**evaluate_model.py**: This file contains functions for evaluating the performance of the trained model, and listening to the outputs generated by the files.

**pipelines.py**: This file contains the NoteSequencePipeline class, which is used to process MIDI files into a format suitable for the transformer model.

**train.py**: This file contains the main training loop for the transformer models.

**transformer.py**: This file contains the implementation of the transformer model used for tokenized MIDI generation.

## Running the Code:
Run the training for the tokenized embedding Transformer:
```console
$ python train.py --verbose --embedding token --data data_400 --tiny
```
Produce output midi files using the tokenized embedding Transformer:
```console
$ python evaluate_model.py --embedding token --data data_400
```
Or using less examples:
```console
$ python evaluate_model.py --embedding token --data data_400 --tiny
```
Equivalently, if you want to try the continuous embedding model:
```console
$ python evaluate_model.py --embedding continuous --data data_400 --tiny
$ python train.py --embedding continuous --data data_400 --tiny
```

### Output Files:
**model_output.mid**: This is a sample output from the trained model.

**outputs/**: This directory contains additional output files from the model, but most are from a previous iteration.

### Model Files:
transformer_midi_model.pth: This is the saved state of the trained transformer model.

